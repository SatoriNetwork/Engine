from typing import Callable, Union, Any, Dict, List, Optional
import copy

# Data processing
# ==============================================================================
import numpy as np
import pandas as pd

from lightgbm import LGBMRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.ensemble import HistGradientBoostingRegressor

# skforecast wrappers/interfaces that simply the use of a combination of different capabilities
from skforecast.ForecasterBaseline import ForecasterEquivalentDate
from skforecast.ForecasterAutoreg import ForecasterAutoreg
from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect
from skforecast.model_selection import bayesian_search_forecaster
from skforecast.model_selection import backtesting_forecaster

# import optuna

from skforecast.ForecasterSarimax import ForecasterSarimax
from skforecast.model_selection_sarimax import backtesting_sarimax
from pmdarima import auto_arima

from skforecast.model_selection import (
    grid_search_forecaster,
    random_search_forecaster,
    bayesian_search_forecaster,
)

# linear regressors : LinearRegression(), Lasso() or Ridge()
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from lineartree import LinearBoostRegressor

from sklearn.preprocessing import StandardScaler

from sktime.forecasting.fbprophet import Prophet
from sktime.forecasting.ets import AutoETS
from sktime.forecasting.tbats import TBATS
from sktime.forecasting.neuralforecast import NeuralForecastLSTM
from sktime.forecasting.model_selection import SlidingWindowSplitter
from sktime.forecasting.model_selection import ForecastingOptunaSearchCV
from sktime.performance_metrics.forecasting import (
    mean_absolute_scaled_error,
    mean_squared_error,
    mean_absolute_error,
)
from sktime.performance_metrics.forecasting import (
    MeanAbsoluteScaledError,
)  # check if this is needed
import optuna


class ForecastModelResult:
    def __init__(
        self,
        model_name: str,
        sampling_freq: str,
        differentiation: int,
        selected_lags: int,
        selected_exog: list,
        dataset_selected_features: pd.DataFrame,
        selected_hyperparameters: Union[Dict[str, Any], None],
        backtest_steps: Optional[int] = None,
        backtest_prediction_interval: Optional[List[int]] = None,
        backtest_predictions: Optional[pd.DataFrame] = None,
        backtest_error: Optional[float] = None,
        backtest_interval_coverage: Optional[Union[float, str]] = None,
        model_trained_on_all_data: Optional[Any] = None,
        forecasting_steps: Optional[int] = None,
        forecast: Optional[pd.DataFrame] = None,
        unfitted_forecaster: Optional[Any] = None,
    ):
        self.model_name = model_name
        self.sampling_freq = sampling_freq
        self.differentiation = differentiation
        self.selected_lags = selected_lags
        self.selected_exog = selected_exog
        self.dataset_selected_features = dataset_selected_features
        self.selected_hyperparameters = selected_hyperparameters
        self.backtest_steps = backtest_steps
        self.backtest_prediction_interval = backtest_prediction_interval
        self.backtest_predictions = backtest_predictions
        self.backtest_error = backtest_error
        self.backtest_interval_coverage = backtest_interval_coverage
        self.model_trained_on_all_data = model_trained_on_all_data
        self.forecasting_steps = forecasting_steps
        self.forecast = forecast
        self.unfitted_forecaster = unfitted_forecaster


def create_forecaster(
    model_type,
    if_exog=None,
    random_state=None,
    verbose=None,
    lags=None,
    differentiation=None,
    custom_params=None,
    weight=None,
    steps=None,
    time_metric_baseline="days",
    forecasterequivalentdate=1,
    forecasterequivalentdate_n_offsets=7,
    y=None,
    start_p=24,
    start_q=0,
    max_p=24,
    max_q=1,
    seasonal=True,
    test="adf",
    m=24,
    d=None,
    D=None,
):
    forecaster_params = {
        "lags": lags,
        "differentiation": (
            differentiation if differentiation and differentiation > 0 else None
        ),
        "weight_func": weight,
        "transformer_y": StandardScaler(),
        "transformer_exog": StandardScaler() if if_exog else None,
    }
    forecaster_params = {k: v for k, v in forecaster_params.items() if v is not None}

    regressor_params = {"random_state": random_state} if random_state else {}

    if custom_params:
        regressor_params.update(custom_params)

    def create_autoreg(regressor_class, **extra_params):
        params = {**regressor_params, **extra_params}
        return lambda: ForecasterAutoreg(
            regressor=regressor_class(**params), **forecaster_params
        )

    def create_autoreg_direct(regressor_class, **extra_params):
        params = {**regressor_params, **extra_params}
        return lambda: ForecasterAutoregDirect(
            regressor=regressor_class(**params), steps=steps, **forecaster_params
        )

    model_creators = {
        "baseline": lambda: ForecasterEquivalentDate(
            offset=pd.DateOffset(**{time_metric_baseline: forecasterequivalentdate}),
            n_offsets=forecasterequivalentdate_n_offsets,
        ),
        "arima": lambda: ForecasterSarimax(
            regressor=auto_arima(
                y=y,
                start_p=start_p,
                start_q=start_q,
                max_p=max_p,
                max_q=max_q,
                seasonal=seasonal,
                test=test or "adf",
                m=m,
                d=d,
                D=D,
                trace=True,
                error_action="ignore",
                suppress_warnings=True,
                stepwise=True,
            ),
            **forecaster_params,
        ),
        "direct_linearregression": create_autoreg_direct(LinearRegression),
        "direct_ridge": create_autoreg_direct(Ridge),
        "direct_lasso": create_autoreg_direct(Lasso),
        "direct_linearboost": create_autoreg_direct(
            LinearBoostRegressor, base_estimator=LinearRegression()
        ),
        "direct_lightgbm": create_autoreg_direct(LGBMRegressor),
        "direct_xgb": create_autoreg_direct(XGBRegressor),
        "direct_catboost": create_autoreg_direct(CatBoostRegressor),
        "direct_histgradient": create_autoreg_direct(HistGradientBoostingRegressor),
        "autoreg_linearregression": create_autoreg(LinearRegression),
        "autoreg_ridge": create_autoreg(Ridge),
        "autoreg_lasso": create_autoreg(Lasso),
        "autoreg_linearboost": create_autoreg(
            LinearBoostRegressor, base_estimator=LinearRegression()
        ),  # test
        "autoreg_lightgbm": create_autoreg(LGBMRegressor, verbose=verbose),
        "autoreg_randomforest": create_autoreg(RandomForestRegressor),
        "autoreg_xgb": create_autoreg(XGBRegressor),
        "autoreg_catboost": create_autoreg(
            CatBoostRegressor,
            verbose=False,
            allow_writing_files=False,
            boosting_type="Plain",
            leaf_estimation_iterations=10,
        ),
        "autoreg_histgradient": create_autoreg(
            HistGradientBoostingRegressor, verbose=0 if verbose == -1 else verbose
        ),
    }

    if model_type not in model_creators:
        raise ValueError(f"Unknown model type: {model_type}")

    if model_type == "arima" and y is None:
        raise ValueError("For ARIMA model, 'y' parameter is required.")

    return model_creators[model_type]()


def perform_backtesting(
    forecaster,
    y,
    end_validation,
    exog=None,
    steps=24,
    metric="mean_absolute_scaled_error",
    refit=False,
    interval=None,
    n_boot=0,
    in_sample_residuals=True,
    binned_residuals=False,
    is_sarimax=False,
    fixed_train_size=False,
    suppress_warnings_fit=True,
):
    print(len(y))
    print(len(y.loc[:end_validation]))
    if is_sarimax:
        backtesting_params = {
            "forecaster": forecaster,
            "y": y,
            "initial_train_size": len(y.loc[:end_validation]),
            "fixed_train_size": fixed_train_size,
            "steps": steps,
            "metric": metric,
            "refit": False,
            "n_jobs": "auto",
            "suppress_warnings_fit": suppress_warnings_fit,
            "verbose": False,
            "show_progress": False,
        }
        backtest_metric, backtest_predictions = backtesting_sarimax(
            **backtesting_params
        )
    else:
        backtesting_params = {
            "forecaster": forecaster,
            "y": y,
            "exog": exog,
            "steps": steps,
            "metric": metric,
            "initial_train_size": len(y.loc[:end_validation]),
            "refit": refit,
            "interval": interval,
            "n_boot": n_boot,
            "in_sample_residuals": in_sample_residuals,
            "binned_residuals": binned_residuals,
            "n_jobs": "auto",
            "verbose": False,
            "show_progress": True,
        }
        backtest_metric, backtest_predictions = backtesting_forecaster(
            **backtesting_params
        )

    print(f"Backtest error ({metric}): {backtest_metric}")
    return backtest_metric, backtest_predictions


class GeneralizedHyperparameterSearch:
    def __init__(
        self,
        forecaster,
        y,
        lags,
        exog=None,
        steps=12,
        metric="mean_absolute_scaled_error",
        initial_train_size=None,
        fixed_train_size=False,
        refit=False,
        return_best=True,
        n_jobs=1, # was "auto" ( consideration for future )
        verbose=False,
        show_progress=True,
    ):
        self.forecaster = forecaster
        self.y = y
        self.exog = exog
        self.steps = steps
        self.metric = metric
        self.initial_train_size = initial_train_size
        self.fixed_train_size = fixed_train_size
        self.refit = refit
        self.return_best = return_best
        self.n_jobs = n_jobs
        self.verbose = verbose
        self.show_progress = show_progress
        self.default_param_ranges = {
            "lightgbm": {
                "n_estimators": ("int", 400, 1200, 100),
                "max_depth": ("int", 3, 10, 1),
                "min_data_in_leaf": ("int", 25, 500),
                "learning_rate": ("float", 0.01, 0.5),
                "feature_fraction": ("float", 0.5, 1, 0.1),
                "max_bin": ("int", 50, 250, 25),
                "reg_alpha": ("float", 0, 1, 0.1),
                "reg_lambda": ("float", 0, 1, 0.1),
                "lags": ("categorical", [lags]),
            },
            "catboost": {
                "n_estimators": ("int", 100, 1000, 100),
                "max_depth": ("int", 3, 10, 1),
                "learning_rate": ("float", 0.01, 1),
                "lags": ("categorical", [lags]),
            },
            "randomforest": {
                "n_estimators": ("int", 400, 1200, 100),
                "max_depth": ("int", 3, 10, 1),
                "ccp_alpha": ("float", 0, 1, 0.1),
                "lags": ("categorical", [lags]),
            },
            "xgboost": {
                "n_estimators": ("int", 30, 5000),
                "max_depth": ("int", -1, 256),
                "learning_rate": ("float", 0.01, 1),
                "subsample": ("float", 0.01, 1.0),
                "colsample_bytree": ("float", 0.01, 1.0),
                "gamma": ("float", 0, 1),
                "reg_alpha": ("float", 0, 1),
                "reg_lambda": ("float", 0, 1),
                "lags": ("categorical", [lags]),
            },
            "histgradient": {
                "max_iter": ("int", 400, 1200, 100),
                "max_depth": ("int", 1, 256),
                "learning_rate": ("float", 0.01, 1),
                "min_samples_leaf": ("int", 1, 20, 1),
                "l2_regularization": ("float", 0, 1),
                "lags": ("categorical", [lags]),
            },
        }
        self.model_type = self._detect_model_type()
        self.current_param_ranges = self.default_param_ranges.get(
            self.model_type, {}
        ).copy()

    def _detect_model_type(self):
        if "LGBMRegressor" in str(type(self.forecaster.regressor)):
            return "lightgbm"
        elif "CatBoostRegressor" in str(type(self.forecaster.regressor)):
            return "catboost"
        elif "RandomForestRegressor" in str(type(self.forecaster.regressor)):
            return "randomforest"
        elif "XGBRegressor" in str(type(self.forecaster.regressor)):
            return "xgboost"
        elif "HistGradientBoostingRegressor" in str(type(self.forecaster.regressor)):
            return "histgradient"
        else:
            return "unknown"

    def exclude_parameters(self, params_to_exclude):
        """
        Exclude specified parameters from the search space.

        :param params_to_exclude: List of parameter names to exclude
        """
        for param in params_to_exclude:
            if param in self.current_param_ranges:
                del self.current_param_ranges[param]
            else:
                print(
                    f"Warning: Parameter '{param}' not found in the current search space."
                )

    def include_parameters(self, params_to_include):
        """
        Include previously excluded parameters back into the search space.

        :param params_to_include: List of parameter names to include
        """
        default_ranges = self.default_param_ranges.get(self.model_type, {})
        for param in params_to_include:
            if param in default_ranges and param not in self.current_param_ranges:
                self.current_param_ranges[param] = default_ranges[param]
            elif param in self.current_param_ranges:
                print(
                    f"Warning: Parameter '{param}' is already in the current search space."
                )
            else:
                print(
                    f"Warning: Parameter '{param}' not found in the default search space for {self.model_type}."
                )

    def update_parameter_range(self, param, new_range):
        """
        Update the range of a specific parameter.

        :param param: Name of the parameter to update
        :param new_range: New range for the parameter (tuple)
        """
        if param in self.current_param_ranges:
            self.current_param_ranges[param] = new_range
        else:
            print(
                f"Warning: Parameter '{param}' not found in the current search space."
            )

    def display_available_parameters(self):
        """
        Display the available parameters and their current ranges for the selected model type.
        """
        print(f"Available parameters for {self.model_type.upper()} model:")
        self._display_params(self.current_param_ranges)
        print(
            "\nYou can override these parameters by passing a dictionary to the bayesian_search method."
        )

    def _display_params(self, param_ranges):
        for param, config in param_ranges.items():
            param_type = config[0]
            if param_type in ["int", "float"]:
                step = config[3] if len(config) > 3 else "N/A"
                print(
                    f"  {param}: {param_type}, range: {config[1]} to {config[2]}, step: {step}"
                )
            elif param_type == "categorical":
                print(f"  {param}: {param_type}, choices: {config[1]}")

    def _prepare_lags_grid(self, lags):
        if isinstance(lags, dict):
            return lags
        elif isinstance(lags, (list, np.ndarray)):
            return {"lags": lags}
        else:
            raise ValueError("lags must be either a dict, list, or numpy array")

    def _prepare_param_grid(self, param_ranges):
        param_grid = {}
        for param, config in param_ranges.items():
            param_type = config[0]
            if param_type in ["int", "float"]:
                start, stop = config[1:3]
                step = config[3] if len(config) > 3 else 1
                if param_type == "int":
                    param_grid[param] = list(range(start, stop + 1, step))
                else:
                    param_grid[param] = list(np.arange(start, stop + step, step))
            elif param_type == "categorical":
                param_grid[param] = config[1]
        return param_grid

    def _prepare_param_distributions(self, param_ranges):
        param_distributions = {}
        for param, config in param_ranges.items():
            param_type = config[0]
            if param_type in ["int", "float"]:
                start, stop = config[1:3]
                step = config[3] if len(config) > 3 else 1
                if param_type == "int":
                    param_distributions[param] = np.arange(
                        start, stop + 1, step, dtype=int
                    )
                else:
                    param_distributions[param] = np.arange(start, stop + step, step)
            elif param_type == "categorical":
                param_distributions[param] = config[1]
        return param_distributions

    def grid_search(self, lags_grid, param_ranges=None):
        if param_ranges is None:
            param_ranges = self.current_param_ranges
        else:
            self.current_param_ranges.update(param_ranges)

        param_grid = self._prepare_param_grid(self.current_param_ranges)
        lags_grid = self._prepare_lags_grid(lags_grid)

        return grid_search_forecaster(
            forecaster=self.forecaster,
            y=self.y,
            exog=self.exog,
            param_grid=param_grid,
            lags_grid=lags_grid,
            steps=self.steps,
            metric=self.metric,
            initial_train_size=self.initial_train_size,
            fixed_train_size=self.fixed_train_size,
            refit=self.refit,
            return_best=self.return_best,
            n_jobs=self.n_jobs,
            verbose=self.verbose,
            show_progress=self.show_progress,
        )

    # needs to be fixed
    def random_search(self, lags_grid, param_ranges=None, n_iter=10, random_state=123):
        if param_ranges is None:
            param_ranges = self.current_param_ranges
        else:
            self.current_param_ranges.update(param_ranges)

        param_distributions = self._prepare_param_distributions(
            self.current_param_ranges
        )

        return random_search_forecaster(
            forecaster=self.forecaster,
            y=self.y,
            exog=self.exog,
            param_distributions=param_distributions,
            lags_grid=lags_grid,
            steps=self.steps,
            n_iter=n_iter,
            metric=self.metric,
            initial_train_size=self.initial_train_size,
            fixed_train_size=self.fixed_train_size,
            refit=self.refit,
            return_best=self.return_best,
            random_state=random_state,
            n_jobs=self.n_jobs,
            verbose=self.verbose,
            show_progress=self.show_progress,
        )

    def bayesian_search(self, param_ranges=None, n_trials=20, random_state=123):
        if param_ranges is None:
            param_ranges = {}

        # Update current_param_ranges with user-provided param_ranges
        for param, range_value in param_ranges.items():
            if param in self.current_param_ranges:
                self.current_param_ranges[param] = range_value
            else:
                self.current_param_ranges[param] = range_value
                print(f"New parameter '{param}' added to the search space.")

        def create_search_space(trial, param_ranges):
            search_space = {}
            for param, config in param_ranges.items():
                param_type = config[0]

                if param_type == "int":
                    start, stop = config[1:3]
                    step = config[3] if len(config) > 3 else 1
                    search_space[param] = trial.suggest_int(
                        param, start, stop, step=step
                    )
                elif param_type == "float":
                    start, stop = config[1:3]
                    step = config[3] if len(config) > 3 else None
                    if step:
                        search_space[param] = trial.suggest_float(
                            param, start, stop, step=step
                        )
                    else:
                        search_space[param] = trial.suggest_float(param, start, stop)
                elif param_type == "categorical":
                    choices = config[1]
                    search_space[param] = trial.suggest_categorical(param, choices)
                else:
                    raise ValueError(
                        f"Unknown parameter type for {param}: {param_type}"
                    )
            return search_space

        def search_space_wrapper(trial):
            return create_search_space(trial, self.current_param_ranges)

        print(random_state)
        return bayesian_search_forecaster(
            forecaster=self.forecaster,
            y=self.y,
            exog=self.exog,
            search_space=search_space_wrapper,
            steps=self.steps,
            metric=self.metric,
            initial_train_size=self.initial_train_size,
            fixed_train_size=self.fixed_train_size,
            refit=self.refit,
            return_best=self.return_best,
            n_trials=n_trials,
            random_state=random_state,
            n_jobs=self.n_jobs,
            verbose=self.verbose,
            show_progress=self.show_progress,
        )


def predict_interval_custom(
    forecaster, steps=24, interval=[10, 90], n_boot=20, exog=None, alpha=None
):
    if isinstance(forecaster, ForecasterSarimax):
        # Case for ARIMA model
        if alpha is None:
            alpha = 0.05  # Default value if not provided
        return forecaster.predict_interval(steps=steps, alpha=alpha)
    elif exog is not None:
        # Case with exogenous features for other models
        return forecaster.predict_interval(
            exog=exog, steps=steps, interval=interval, n_boot=n_boot
        )
    else:
        # Case without exogenous features for other models
        return forecaster.predict_interval(
            steps=steps, interval=interval, n_boot=n_boot
        )


def calculate_theoretical_coverage(interval):
    if len(interval) != 2:
        raise ValueError("Interval must contain exactly two values")

    lower, upper = interval
    if not (isinstance(lower, (int, float)) and isinstance(upper, (int, float))):
        raise ValueError("Interval values must be numbers")

    if lower >= upper:
        raise ValueError("Lower bound must be less than upper bound")

    coverage = upper - lower
    # print(f"Theoretical coverage: {coverage}%")
    return coverage


def calculate_interval_coverage(
    satoridataset, interval_predictions, end_validation, end_test, value
):
    # Ensure both datasets have the same index
    common_index = satoridataset.loc[end_validation:end_test].index.intersection(
        interval_predictions.index
    )

    # Align the datasets
    satori_aligned = satoridataset.loc[common_index, value]
    predictions_aligned = interval_predictions.loc[common_index]

    # Calculate coverage
    coverage = np.mean(
        np.logical_and(
            satori_aligned >= predictions_aligned["lower_bound"],
            satori_aligned <= predictions_aligned["upper_bound"],
        )
    )

    # pseudo-code
    # a df which contains a common index [date-time], 3 columns[  predictions_aligned["lower_bound"], satori_aligned["value"], predictions_aligned["upper_bound"] ]
    coverage_df = pd.DataFrame(
        {
            "lower_bound": predictions_aligned["lower_bound"],
            value: satori_aligned,
            "upper_bound": predictions_aligned["upper_bound"],
            "if_in_range": (satori_aligned >= predictions_aligned["lower_bound"])
            & (satori_aligned <= predictions_aligned["upper_bound"]),
        },
        index=common_index,
    )

    total_count = len(coverage_df)
    in_range_count = coverage_df["if_in_range"].sum()
    # end

    # Equivalent code
    # covered_data = 0
    # total_rows = len(satori_aligned)

    # for i in range(total_rows):
    #     aligned_value = satori_aligned.iloc[i]
    #     lower_bound = predictions_aligned['lower_bound'].iloc[i]
    #     upper_bound = predictions_aligned['upper_bound'].iloc[i]

    #     if (aligned_value >= lower_bound) and (aligned_value <= upper_bound):
    #         covered_data += 1

    # coverage = covered_data / total_rows
    # end Equivalent code

    # Calculate total area of the interval
    area = (
        predictions_aligned["upper_bound"] - predictions_aligned["lower_bound"]
    ).sum()

    print(f"Total data points: {total_count}")
    print(f"Data points within range: {in_range_count}")
    print(
        f"Predicted interval coverage assuming Gaussian distribution: {round(100*coverage, 2)}%"
    )
    print(f"Total area of the interval: {round(area, 2)}")
    # print(coverage_df)

    return coverage, area


def calculate_error(y_true, backtest_prediction, y_train, error_type="mase"):
    """
    Calculate either Mean Absolute Scaled Error (MASE), Mean Squared Error (MSE),
    or Mean Absolute Error (MAE) based on the specified error_type for forecast data.

    Parameters:
    y_true (pandas.Series): True values
    backtest_prediction (pandas.DataFrame): DataFrame containing predictions and intervals
    y_train (pandas.Series): Training data used for scaling in MASE
    error_type (str): Type of error to calculate ('mase', 'mse', or 'mae')

    Returns:
    float: Calculated error value
    """
    # Ensure y_true and y_train are pandas Series with a datetime index
    if not isinstance(y_true, pd.Series):
        raise ValueError("y_true must be a pandas Series with a datetime index")
    if not isinstance(y_train, pd.Series):
        raise ValueError("y_train must be a pandas Series with a datetime index")

    # Align y_true with backtest_prediction
    aligned_true = y_true.loc[backtest_prediction.index]

    # Extract point predictions
    y_pred = backtest_prediction["pred"]

    if error_type.lower() == "mase":
        return mean_absolute_scaled_error(aligned_true, y_pred, y_train=y_train)
    elif error_type.lower() == "mse":
        return mean_squared_error(aligned_true, y_pred)
    elif error_type.lower() == "mae":
        return mean_absolute_error(aligned_true, y_pred)
    else:
        raise ValueError("Invalid error_type. Choose 'mase', 'mse', or 'mae'.")


def model_create_train_test_and_predict(
    model_name: str,
    dataset: pd.DataFrame,
    dataset_train: pd.DataFrame,
    end_validation: pd.Timestamp,
    end_test: pd.Timestamp,
    sampling_freq: str,
    differentiation: int,
    selected_lags: int,
    selected_exog: list,
    dataset_selected_features: pd.DataFrame,
    data_missing: bool,
    weight: Union[Callable[[pd.DatetimeIndex], np.ndarray], None],
    baseline_1: str,
    baseline_2: int,
    baseline_3: int,
    select_hyperparameters: bool = True,
    default_hyperparameters: Union[Dict[str, Any], None] = None,
    random_state_hyper: int = 123,
    backtest_steps: int = 24,
    interval: List[int] = [10, 90],
    metric: str = "mase",
    forecast_calendar_features: Union[List[str], None] = None,
    forecasting_steps: int = 24,
    hour_seasonality: bool = False,
    dayofweek_seasonality: bool = False,
    week_seasonality: bool = False,
    mode: str = "train",
    forecaster: Optional[Any] = None,
):

    if data_missing:
        value = "value_imputed"
    else:
        value = "value"

    y = dataset_selected_features[value].copy()
    y.index = pd.to_datetime(y.index)
    split_index = y.index.get_loc(end_validation)
    y_train = y.iloc[: split_index + 1]
    y_train.index = pd.to_datetime(y_train.index)
    y_test = y.iloc[split_index + 1 :]

    if model_name.lower() == "baseline":
        baseline_forecaster = create_forecaster(
            "baseline",
            time_metric_baseline=baseline_1,
            forecasterequivalentdate=baseline_2,
            forecasterequivalentdate_n_offsets=baseline_3,
        )

        backtest_predictions = None
        backtest_error = None
        coverage = None
        forecast = None

        if mode == "all" or mode == "train":
            baseline_forecaster.fit(
                y=dataset_selected_features.loc[:end_validation, value]
            )
            backtest_predictions = baseline_forecaster.predict(steps=backtest_steps)
            backtest_predictions = backtest_predictions.to_frame(name="pred")
            backtest_error = calculate_error(
                y_test,
                backtest_predictions[: len(y_test)],
                y_train,
                error_type=metric,
            )
            coverage = "NA"

        if mode == "all" or mode == "predict":
            baseline_forecaster.fit(y=dataset_selected_features[value])
            forecast = baseline_forecaster.predict(steps=forecasting_steps)

        return ForecastModelResult(
            model_name=model_name,
            sampling_freq=sampling_freq,
            differentiation=differentiation,
            selected_lags=selected_lags,
            selected_exog=selected_exog,
            dataset_selected_features=dataset_selected_features,
            selected_hyperparameters=None,
            backtest_steps=backtest_steps,
            backtest_prediction_interval=interval,
            backtest_predictions=backtest_predictions,
            backtest_error=backtest_error,
            backtest_interval_coverage=coverage,
            model_trained_on_all_data=baseline_forecaster,
            forecasting_steps=forecasting_steps,
            forecast=forecast,
        )

    elif model_name.lower() == "arima":
        sampling_timedelta = pd.Timedelta(sampling_freq)
        day_timedelta = pd.Timedelta(days=1)
        
        seasonal = False
        if hour_seasonality:
            m = forecasting_steps
            seasonal = True
        elif dayofweek_seasonality:
            if sampling_timedelta <= pd.Timedelta(hours=1):
                m = forecasting_steps * 7
            else:
                multiplier = max(1, int(day_timedelta / sampling_timedelta))
                m = 7 * multiplier
            seasonal = True
        else:
            m = 1

        m = int(m)
        print(m)

        arima_forecaster = create_forecaster(
            model_type="arima",
            y=dataset_selected_features.loc[:end_validation, value],
            start_p=forecasting_steps,
            start_q=0,
            max_p=forecasting_steps,
            max_q=2, # was 1
            seasonal=seasonal,
            test="adf",
            m=m,
            d=None,
            D=None,
        )

        backtest_predictions = None
        backtest_error = None
        coverage = None
        forecast = None

        if mode == "all" or mode == "train":
            arima_forecaster.fit(
                y=dataset_selected_features.loc[:end_validation, value],
                suppress_warnings=True
            )
            backtest_predictions = arima_forecaster.predict_interval(
                steps=backtest_steps, interval=interval
            )
            backtest_error = calculate_error(
                y_test,
                backtest_predictions[: len(y_test)],
                y_train,
                error_type=metric,
            )
            coverage, _ = calculate_interval_coverage(
                dataset_selected_features,
                backtest_predictions,
                end_validation,
                end_test,
                value,
            )

        if mode == "all" or mode == "predict":
            arima_forecaster.fit(y=dataset_selected_features[value], suppress_warnings=True)
            forecast = predict_interval_custom(
                forecaster=arima_forecaster, steps=forecasting_steps, alpha=0.05
            )

        return ForecastModelResult(
            model_name=model_name,
            sampling_freq=sampling_freq,
            differentiation=differentiation,
            selected_lags=selected_lags,
            selected_exog=selected_exog,
            dataset_selected_features=dataset_selected_features,
            selected_hyperparameters=None,  # ARIMA doesn't use hyperparameters in this implementation
            backtest_steps=backtest_steps,
            backtest_prediction_interval=interval,
            backtest_predictions=backtest_predictions,
            backtest_error=backtest_error,
            backtest_interval_coverage=coverage,
            model_trained_on_all_data=arima_forecaster,
            forecasting_steps=forecasting_steps,
            forecast=forecast,
        )

    elif model_name[:3].lower() == "skt":

        if model_name.lower() == "skt_lstm_deeplearning":
            forecaster = NeuralForecastLSTM(freq=sampling_freq, max_steps=200)

        elif model_name.lower() == "skt_prophet_additive":
            forecaster = Prophet(
                seasonality_mode="additive",
                daily_seasonality="auto",
                weekly_seasonality="auto",
                yearly_seasonality="auto",
            )

        elif model_name.lower() == "skt_prophet_hyper":
            param_grid = {
                "growth": optuna.distributions.CategoricalDistribution(
                    ["linear", "logistic"]
                ),
                "n_changepoints": optuna.distributions.IntDistribution(5, 20),
                "changepoint_range": optuna.distributions.FloatDistribution(0.7, 0.9),
                "seasonality_mode": optuna.distributions.CategoricalDistribution(
                    ["additive", "multiplicative"]
                ),
                "seasonality_prior_scale": optuna.distributions.LogUniformDistribution(
                    0.01, 10.0
                ),
                "changepoint_prior_scale": optuna.distributions.LogUniformDistribution(
                    0.001, 0.5
                ),
                "holidays_prior_scale": optuna.distributions.LogUniformDistribution(
                    0.01, 10.0
                ),
                "daily_seasonality": optuna.distributions.CategoricalDistribution(
                    ["auto"]
                ),
                "weekly_seasonality": optuna.distributions.CategoricalDistribution(
                    ["auto"]
                ),
                "yearly_seasonality": optuna.distributions.CategoricalDistribution(
                    ["auto"]
                ),
            }

            forecaster_initial = Prophet()

            # Set up a more efficient time series cross-validation
            cv = SlidingWindowSplitter(
                initial_window=int(
                    len(y_train) * 0.6
                ),  # Use 60% of data for initial training
                step_length=int(len(y_train) * 0.2),  # Move forward by 20% each time
                fh=np.arange(1, forecasting_steps + 1),  # Forecast horizon of 12 steps
            )

            fos = ForecastingOptunaSearchCV(
                forecaster=forecaster_initial,
                param_grid=param_grid,
                cv=cv,
                n_evals=50,
                strategy="refit",
                scoring=MeanAbsoluteScaledError(sp=1),
                verbose=1,
            )

            fos.fit(y_train)
            forecaster = fos.best_forecaster_

        elif (
            model_name.lower() == "skt_ets"
        ):  # faster implementation available and should be implemented in the future
            print("entered")
            forecaster = AutoETS(
                error="add",
                trend=None,
                damped_trend=False,
                seasonal=None,
                sp=forecasting_steps,
                initialization_method="estimated",
                initial_level=None,
                initial_trend=None,
                initial_seasonal=None,
                bounds=None,
                dates=None,
                freq=None,
                missing="none",
                start_params=None,
                maxiter=1000,
                full_output=True,
                disp=False,
                callback=None,
                return_params=False,
                auto=True,
                information_criterion="aic",
                allow_multiplicative_trend=True,
                restrict=True,
                additive_only=False,
                ignore_inf_ic=True,
                # n_jobs=-1,
                random_state=random_state_hyper,
            )

        elif model_name[:9].lower() == "skt_tbats":
            splist = []
            sampling_timedelta = pd.Timedelta(sampling_freq)
            day_timedelta = pd.Timedelta(days=1)
            use_box_cox = None
            if model_name.lower() == "skt_tbats_quick":
                use_box_cox = False
            if differentiation == 0:
                use_trend = False
                use_damped_trend = False
            else:
                use_trend = True
                if model_name.lower() == "skt_tbats_damped":
                    use_damped_trend = True
                else:
                    use_damped_trend = False

            print("testing")
            # print(use_trend)
            # print(use_damped_trend)
            # print(use_box_cox)
            if hour_seasonality == True:
                splist.append(forecasting_steps)

            if dayofweek_seasonality == True:
                print("inside")
                if sampling_timedelta <= pd.Timedelta(hours=1):
                    multiplier = 7
                    splist.append(forecasting_steps * multiplier)
                else:
                    multiplier = max(1, int(day_timedelta / sampling_timedelta))
                    splist.append(7 * multiplier)

            if (
                week_seasonality == True
            ):  # calculation of week seasonality test to determine seasonal period of a year can be improved in the future by using day_of_year instead of week_of_year
                print("inside")
                if sampling_timedelta <= pd.Timedelta(hours=1):
                    multiplier = 365.25
                    splist.append(forecasting_steps * multiplier)
                else:
                    multiplier = max(1, int(day_timedelta / sampling_timedelta))
                    splist.append(365.25 * multiplier)

            print(splist)
            forecaster = TBATS(
                use_box_cox=use_box_cox,
                box_cox_bounds=(0, 1),
                use_trend=use_trend,
                use_damped_trend=use_damped_trend,
                sp=splist,
                use_arma_errors=True,
                show_warnings=False,
                # n_jobs=-1,
                multiprocessing_start_method="spawn",
                context=None,
            )

        rounded_index = y_test.index.floor(sampling_freq)
        difference_minutes = (y_test.index - rounded_index).total_seconds() / 60
        y.index = y.index.floor(sampling_freq)
        y_test.index = y_test.index.floor(sampling_freq)
        y_train.index = y_train.index.floor(sampling_freq)
        time_delta = pd.Timedelta(minutes=float(difference_minutes[0]))

        lower, upper = interval
        backtest_prediction = None
        error = None
        coverage = None
        forecast = None

        if mode == "all" or mode == "train":
            if model_name.lower() == "skt_lstm_deeplearning":
                forecaster.fit(y_train, fh=list(range(1, backtest_steps + 1)))
                y_pred_backtest = forecaster.predict(list(range(1, backtest_steps + 1)))
                backtest_prediction = y_pred_backtest.to_frame(name="pred")
                error = calculate_error(
                    y_test, backtest_prediction[: len(y_test)], y_train, error_type=metric
                )
                backtest_prediction.index = backtest_prediction.index + time_delta
                coverage = "NA"
            else:
                forecaster.fit(y_train)
                y_pred_backtest = forecaster.predict(list(range(1, backtest_steps + 1)))
                
                y_pred_backtest_interval = forecaster.predict_interval(
                    fh=list(range(1, backtest_steps + 1)), coverage=[(upper - lower) / 100]
                )
                y_pred_backtest_df = y_pred_backtest.to_frame(name="pred")
                backtest_prediction = pd.concat(
                    [y_pred_backtest_df, y_pred_backtest_interval], axis=1
                )
                backtest_prediction.columns = ["pred", "lower_bound", "upper_bound"]
                error = calculate_error(
                    y_test, backtest_prediction[: len(y_test)], y_train, error_type=metric
                )
                backtest_prediction.index = backtest_prediction.index + time_delta
                coverage, _ = calculate_interval_coverage(
                    dataset_selected_features,
                    backtest_prediction,
                    end_validation,
                    end_test,
                    value,
                )

        if mode == "all" or mode == "predict":
            if model_name.lower() == "skt_lstm_deeplearning":
                forecaster.fit(y, fh=list(range(1, forecasting_steps + 1)))
                y_pred_future = forecaster.predict(list(range(1, forecasting_steps + 1)))
                forecast = y_pred_future.to_frame(name="pred")
                forecast.index = forecast.index + time_delta
            else:
                forecaster.fit(y)
                y_pred_future = forecaster.predict(list(range(1, forecasting_steps + 1)))
                y_pred_future_interval = forecaster.predict_interval(
                    fh=list(range(1, forecasting_steps + 1)),
                    coverage=[(upper - lower) / 100],
                )
                y_pred_df = y_pred_future.to_frame(name="pred")
                forecast = pd.concat([y_pred_df, y_pred_future_interval], axis=1)
                forecast.columns = ["pred", "lower_bound", "upper_bound"]
                forecast.index = forecast.index + time_delta

        return ForecastModelResult(
            model_name=model_name,
            sampling_freq=sampling_freq,
            differentiation=differentiation,
            selected_lags=selected_lags,
            selected_exog=selected_exog,
            dataset_selected_features=dataset_selected_features,
            selected_hyperparameters=None,  # SKT models don't use hyperparameters in this implementation
            backtest_steps=backtest_steps,
            backtest_prediction_interval=interval,
            backtest_predictions=backtest_prediction,
            backtest_error=error,
            backtest_interval_coverage=coverage,
            model_trained_on_all_data=forecaster,
            forecasting_steps=forecasting_steps,
            forecast=forecast,
        )

    else:

        if model_name[:6].lower() == "direct":

            if model_name == "direct_linearregression":
                random_state = None
            else:
                random_state = 123
            verbose = None
            differentiation = None
            steps = forecasting_steps

        else:

            if model_name == "autoreg_linearregression":
                random_state = None
            else:
                random_state = 123
            verbose = -1
            differentiation = differentiation

            if model_name == "autoreg_linearboost":
                differentiation = None
            steps = None

        if selected_exog == []:
            if_exog = None
        else:
            if_exog = StandardScaler()

        backtest_predictions = None
        backtest_error = None
        coverage = None
        forecast = None
        unfitted_forecaster = None
        selected_hyperparameters = None

        if mode == "all" or mode == "train":
        # Create forecaster
            forecaster = create_forecaster(
                model_name,
                random_state=random_state,
                verbose=verbose,
                lags=selected_lags,
                steps=steps,
                weight=weight,
                differentiation=differentiation,
                if_exog=if_exog,
            )

            if model_name.lower() != "direct_linearregression":
                # # Hyperparameter search if required
                if select_hyperparameters:
                    forecaster_search = GeneralizedHyperparameterSearch(
                        forecaster=forecaster,
                        y=dataset_selected_features.loc[:end_validation, value],
                        lags=selected_lags,
                        exog=dataset_selected_features.loc[:end_validation, selected_exog],
                        steps=forecasting_steps,
                        initial_train_size=len(dataset_train),  # dataset with features
                        # metric=metric,
                    )
                    results_search, _ = forecaster_search.bayesian_search(
                        param_ranges=default_hyperparameters,
                        n_trials=20,
                        random_state=random_state_hyper,
                    )
                    selected_hyperparameters = results_search["params"].iat[0]
                else:
                    selected_hyperparameters = default_hyperparameters
            else:
                selected_hyperparameters = None

            # # Create final forecaster with best parameters
            final_forecaster = create_forecaster(
                model_name,
                random_state=random_state,
                verbose=verbose,
                lags=selected_lags,
                steps=steps,
                weight=weight,
                differentiation=differentiation,
                custom_params=selected_hyperparameters,
                if_exog=if_exog,
            )

            unfitted_forecaster = copy.deepcopy(final_forecaster)

            if len(selected_exog) == 0:
                final_forecaster.fit(
                    y=dataset_selected_features.loc[:end_validation, value]
                )
            else:
                final_forecaster.fit(
                    y=dataset_selected_features.loc[:end_validation, value],
                    exog=dataset_selected_features.loc[:end_validation, selected_exog],
                )

            if len(selected_exog) == 0:
                backtest_predictions = final_forecaster.predict_interval(
                    steps=backtest_steps, interval=interval
                )
            else:
                backtest_predictions = final_forecaster.predict_interval(
                    steps=backtest_steps,
                    exog=dataset_selected_features.loc[
                        dataset_selected_features.index > end_validation, selected_exog
                    ],
                    interval=interval,
                )

            backtest_error = calculate_error(
                y_test, backtest_predictions[: len(y_test)], y_train, error_type=metric
            )

            coverage, _ = calculate_interval_coverage(
                dataset_selected_features,
                backtest_predictions,
                end_validation,
                end_test,
                value,
            )

        if mode == "all" or mode == "predict":
            final_forecaster = forecaster
            if len(selected_exog) == 0:
                final_forecaster.fit(y=dataset_selected_features[value])
            else:
                final_forecaster.fit(
                    y=dataset_selected_features[value],
                    exog=dataset_selected_features.loc[:end_test, selected_exog],
                )

            if len(selected_exog) == 0:
                exog = None
            else:
                exog = forecast_calendar_features[selected_exog]

            # Make the forecast
            forecast = predict_interval_custom(
                forecaster=final_forecaster,
                exog=exog,
                steps=forecasting_steps,
                interval=interval,
                n_boot=20,
            )

        return ForecastModelResult(
            model_name=model_name,
            sampling_freq=sampling_freq,
            differentiation=differentiation,
            selected_lags=selected_lags,
            selected_exog=selected_exog,
            dataset_selected_features=dataset_selected_features,
            selected_hyperparameters=selected_hyperparameters,
            backtest_steps=backtest_steps,
            backtest_prediction_interval=interval,
            backtest_predictions=backtest_predictions,
            backtest_error=backtest_error,
            backtest_interval_coverage=coverage,
            model_trained_on_all_data=final_forecaster,
            unfitted_forecaster=unfitted_forecaster,
            forecasting_steps=forecasting_steps,
            forecast=forecast,
        )
